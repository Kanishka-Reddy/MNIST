{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1I_tUWJaagZS5Zj9JyP-mJ8ny3rTM7sHW",
      "authorship_tag": "ABX9TyNTXEv/BQhg0kVOzMM1KsqZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kanishka-Reddy/MNIST/blob/main/RNN-Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieZZ1Jne1YO4",
        "outputId": "1cc26054-740e-4607-89b1-b56dbb260531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ncps in /usr/local/lib/python3.8/dist-packages (0.0.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from ncps) (0.16.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from ncps) (23.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ncps\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy, time\n",
        "import numpy as np\n",
        "\n",
        "from ncps.torch import CfC, LTC\n",
        "from ncps.wirings import AutoNCP\n",
        "\n",
        "\n",
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module.\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "    def __init__(self, d_model, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        d_ff = d_model * 4\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
        "\n",
        "\n",
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"\"\"\n",
        "        Compute 'Scaled Dot Product Attention'\n",
        "        query, key, value : batch_size, n_head, seq_len, dim of space\n",
        "    \"\"\"\n",
        " \n",
        "    d_k = query.size(-1)\n",
        "    # scores: batch_size, n_head, seq_len, seq_len\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "             / math.sqrt(d_k)\n",
        "   \n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask==0, -1e9)\n",
        "\n",
        "\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "\n",
        "class MHPooling(nn.Module):\n",
        "    def __init__(self, d_model, h, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MHPooling, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        #auto-regressive\n",
        "        attn_shape = (1, 3000, 3000)\n",
        "        subsequent_mask =  np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "        self.mask = (torch.from_numpy(subsequent_mask) == 0).unsqueeze(1).cuda()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"Implements Figure 2\"\n",
        "\n",
        "        nbatches, seq_len, d_model = x.shape\n",
        "        \n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
        "        query, key, value = \\\n",
        "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (x, x, x))]\n",
        "        \n",
        "        # 2) Apply attention on all the projected vectors in batch. \n",
        "        x, self.attn = attention(query, key, value, mask=self.mask[:,:, :seq_len, :seq_len], \n",
        "                                 dropout=self.dropout)\n",
        "        \n",
        "        # 3) \"Concat\" using a view and apply a final linear. \n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "             .view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)\n",
        "\n",
        "class LocalRNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, rnn_type, ksize, dropout):\n",
        "        super(LocalRNN, self).__init__()\n",
        "        \"\"\"\n",
        "        LocalRNN structure\n",
        "        \"\"\"\n",
        "        self.ksize = ksize\n",
        "        self.wiring = AutoNCP(36, output_dim)  # 36 neurons, output_dim outputs\n",
        "\n",
        "        # if rnn_type == 'LTC':\n",
        "        #     self.rnn = LTC(input_size=output_dim, units=self.wiring, batch_first=True)\n",
        "        # else:\n",
        "        #     self.rnn = CfC(input_size=output_dim, units=self.wiring, batch_first=True)\n",
        "\n",
        "        if rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(output_dim, output_dim, batch_first=True)\n",
        "        elif rnn_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(output_dim, output_dim, batch_first=True)\n",
        "        else:\n",
        "            self.rnn = nn.RNN(output_dim, output_dim, batch_first=True)\n",
        "\n",
        "        self.output = nn.Sequential(nn.Linear(output_dim, output_dim), nn.ReLU())\n",
        "\n",
        "        # To speed up\n",
        "        idx = [i for j in range(self.ksize-1,10000,1) for i in range(j-(self.ksize-1),j+1,1)]\n",
        "        self.select_index = torch.LongTensor(idx).cuda()\n",
        "        self.zeros = torch.zeros((self.ksize-1, input_dim)).cuda()\n",
        "\n",
        "    def forward(self, x):\n",
        "        nbatches, l, input_dim = x.shape\n",
        "        x = self.get_K(x) # b x seq_len x ksize x d_model\n",
        "        batch, l, ksize, d_model = x.shape\n",
        "        h = self.rnn(x.view(-1, self.ksize, d_model))[0][:,-1,:]\n",
        "        return h.view(batch, l, d_model)\n",
        "\n",
        "    def get_K(self, x):\n",
        "        batch_size, l, d_model = x.shape\n",
        "        zeros = self.zeros.unsqueeze(0).repeat(batch_size, 1, 1)\n",
        "        x = torch.cat((zeros, x), dim=1)\n",
        "        key = torch.index_select(x, 1, self.select_index[:self.ksize*l])\n",
        "        key = key.reshape(batch_size, l, self.ksize, -1)\n",
        "        return key\n",
        "\n",
        "\n",
        "class LocalRNNLayer(nn.Module):\n",
        "    \"Encoder is made up of attconv and feed forward (defined below)\"\n",
        "    def __init__(self, input_dim, output_dim, rnn_type, ksize, dropout):\n",
        "        super(LocalRNNLayer, self).__init__()\n",
        "        self.local_rnn = LocalRNN(input_dim, output_dim, rnn_type, ksize, dropout)\n",
        "        self.connection = SublayerConnection(output_dim, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"Follow Figure 1 (left) for connections.\"\n",
        "        x = self.connection(x, self.local_rnn)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"\n",
        "    One Block\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim, rnn_type, ksize, N, h, dropout):\n",
        "        super(Block, self).__init__()\n",
        "        self.layers = clones(\n",
        "            LocalRNNLayer(input_dim, output_dim, rnn_type, ksize, dropout), N)\n",
        "        self.connections = clones(SublayerConnection(output_dim, dropout), 2)\n",
        "        self.pooling = MHPooling(input_dim, h, dropout)\n",
        "        self.feed_forward = PositionwiseFeedForward(input_dim, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        n, l, d = x.shape\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "        x = self.connections[0](x, self.pooling)\n",
        "        x = self.connections[1](x, self.feed_forward)\n",
        "        return x\n",
        "    \n",
        "\n",
        "\n",
        "class RTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    The overall model\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, rnn_type, ksize, n_level, n, h, dropout):\n",
        "        super(RTransformer, self).__init__()\n",
        "        N = n\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = LayerNorm(d_model)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, dropout)\n",
        "        \n",
        "        layers = []\n",
        "        for i in range(n_level):\n",
        "            layers.append(\n",
        "                Block(d_model, d_model, rnn_type, ksize, N=N, h=h, dropout=dropout))\n",
        "        self.forward_net = nn.Sequential(*layers) \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_net(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import os, sys\n",
        "from torch import nn\n",
        "base_path = os.path.dirname(os.path.realpath(sys.executable))\n",
        "sys.path.append(os.path.join(base_path,'../models'))\n",
        "\n",
        "\n",
        "class RT(nn.Module):\n",
        "    def __init__(self, input_size, d_model, output_size, h, rnn_type, ksize, n_level, n, dropout=0.2, emb_dropout=0.2):\n",
        "        super(RT, self).__init__()\n",
        "        self.encoder = nn.Linear(input_size, d_model)\n",
        "        self.rt = RTransformer(d_model, rnn_type, ksize, n_level, n, h, dropout)\n",
        "        self.linear = nn.Linear(d_model, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Inputs have to have dimension (N, C_in, L_in)\"\"\"\n",
        "        x = x.transpose(-2,-1)\n",
        "        x = self.encoder(x)\n",
        "        x = self.rt(x)  # input should have dimension (N, C, L)\n",
        "        x = x.transpose(-2,-1)\n",
        "        o = self.linear(x[:, :, -1])\n",
        "        return F.log_softmax(o, dim=1)"
      ],
      "metadata": {
        "id": "DsCU1WRi17WW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "def data_generator(root, batch_size):\n",
        "    train_set = datasets.MNIST(root=root, train=True, download=True,\n",
        "                               transform=transforms.Compose([\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize((0.1307,), (0.3081,))\n",
        "                               ]))\n",
        "    test_set = datasets.MNIST(root=root, train=False, download=True,\n",
        "                              transform=transforms.Compose([\n",
        "                                  transforms.ToTensor(),\n",
        "                                  transforms.Normalize((0.1307,), (0.3081,))\n",
        "                              ]))\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
        "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "PBhdZL932zba"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "dir_path = os.path.dirname(os.path.realpath(sys.executable))\n",
        "print(dir_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MilrHubK3jCI",
        "outputId": "a015ee90-86a8-4a83-dd03-e4178196a37b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "  \n",
        "# Directory\n",
        "directory = \"output\"\n",
        "d2 = \"data\"\n",
        "  \n",
        "# Parent Directory path\n",
        "parent_dir = \"/usr/bin\"\n",
        "  \n",
        "# Path\n",
        "path = os.path.join(parent_dir, directory)\n",
        "path2 = os.path.join(parent_dir, d2)\n",
        "# Create the directory\n",
        "# 'GeeksForGeeks' in\n",
        "# '/home / User / Documents'\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "  \n",
        "if not os.path.exists(path2):\n",
        "    os.makedirs(path2)\n",
        "\n"
      ],
      "metadata": {
        "id": "v1uGmptR4AJX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import sys, os\n",
        "sys.path.append(\"../../\")\n",
        "\n",
        "import numpy as np\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--cuda', action='store_false')\n",
        "parser.add_argument('--batch_size', type=int, default=16)\n",
        "parser.add_argument('--dropout', type=float, default=0.05)\n",
        "parser.add_argument('--clip', type=float, default=-1,\n",
        "                    help='gradient clip, -1 means no clip (default: -1)')\n",
        "parser.add_argument('--epochs', type=int, default=5)\n",
        "parser.add_argument('--ksize', type=int, default=7)\n",
        "parser.add_argument('--n_level', type=int, default=4)\n",
        "parser.add_argument('--log-interval', type=int, default=100, metavar='N')\n",
        "parser.add_argument('--lr', type=float, default=0.001)\n",
        "parser.add_argument('--optim', type=str, default='Adam')\n",
        "parser.add_argument('--rnn_type', type=str, default='LTC')\n",
        "parser.add_argument('--d_model', type=int, default=32)\n",
        "parser.add_argument('--n', type=int, default=2)\n",
        "parser.add_argument('--h', type=int, default=2)\n",
        "parser.add_argument('--seed', type=int, default=1111)\n",
        "parser.add_argument('--permute', action='store_true', default=False)\n",
        "\n",
        "# args = parser.parse_args()\n",
        "args, unknown = parser.parse_known_args()\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "base_path = os.path.dirname(os.path.realpath(sys.executable))\n",
        "root = os.path.join(base_path,'data')\n",
        "s_dir = os.path.join(base_path,'output/')\n",
        "\n",
        "\n",
        "batch_size = args.batch_size\n",
        "n_classes = 10\n",
        "input_channels = 1\n",
        "seq_length = int(784 / input_channels)\n",
        "epochs = args.epochs\n",
        "steps = 0\n",
        "print(args)\n",
        "\n",
        "train_loader, test_loader = data_generator(root, batch_size)\n",
        "\n",
        "model = RT(input_channels, args.d_model, n_classes, h=args.h, rnn_type=args.rnn_type, ksize=args.ksize, \n",
        "    n_level=args.n_level, n=args.n, dropout=args.dropout, emb_dropout=args.dropout)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "model_name = \"d_{}_h_{}_t_{}_ksize_{}_level_{}_n_{}_lr_{}_dropout_{}\".format(\n",
        "            args.d_model, args.h, args.rnn_type, args.ksize, \n",
        "            args.n_level, args.n, args.lr, args.dropout)\n",
        "\n",
        "newpath = r'C:\\Program Files\\arbitrary' \n",
        "if not os.path.exists(newpath):\n",
        "    os.makedirs(newpath)\n",
        "  \n",
        "message_filename = s_dir + 'r_' + model_name + '.txt'\n",
        "model_filename = s_dir + 'm_' + model_name + '.pt'\n",
        "with open(message_filename, 'w') as out:\n",
        "    out.write('start\\n')\n",
        "\n",
        "\n",
        "lr = args.lr\n",
        "optimizer = getattr(optim, args.optim)(model.parameters(), lr=lr)\n",
        "\n",
        "def save(model, save_filename):\n",
        "    with open(save_filename, \"wb\") as f:\n",
        "        torch.save(model, f)\n",
        "    print('Saved as %s' % save_filename)\n",
        "\n",
        "\n",
        "def output_s(message, save_filename):\n",
        "    print (message)\n",
        "    with open(save_filename, 'a') as out:\n",
        "        out.write(message + '\\n')\n",
        "\n",
        "def train(ep):\n",
        "    global steps\n",
        "    train_loss = 0\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if args.cuda: data, target = data.cuda(), target.cuda()\n",
        "        data = data.view(-1, input_channels, seq_length)\n",
        "        if args.permute:\n",
        "            data = data[:, :, permute]\n",
        "        data, target = data, target\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        if args.clip > 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
        "        optimizer.step()\n",
        "        train_loss += loss\n",
        "        steps += seq_length\n",
        "        if batch_idx > 0 and batch_idx % args.log_interval == 0:\n",
        "            message = ('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tSteps: {}'.format(\n",
        "                ep, batch_idx * batch_size, len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), train_loss.item()/args.log_interval, steps))\n",
        "            output_s(message, message_filename)\n",
        "            train_loss = 0\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            if args.cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            data = data.view(-1, input_channels, seq_length)\n",
        "            if args.permute:\n",
        "                data = data[:, :, permute]\n",
        "            data, target = data,  target\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        message = ('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            test_loss, correct, len(test_loader.dataset),\n",
        "            100. * correct / len(test_loader.dataset)))\n",
        "        output_s(message, message_filename)\n",
        "        return test_loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for epoch in range(1, epochs+1):\n",
        "        train(epoch)\n",
        "        test()\n",
        "        if epoch % 10 == 0:\n",
        "            lr /= 10\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zszHW5vo26hy",
        "outputId": "3f00e239-3380-424d-e993-a2af13030212"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(batch_size=16, clip=-1, cuda=True, d_model=32, dropout=0.05, epochs=5, h=2, ksize=7, log_interval=100, lr=0.001, n=2, n_level=4, optim='Adam', permute=False, rnn_type='LTC', seed=1111)\n",
            "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 2.423717\tSteps: 79184\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.215272\tSteps: 157584\n",
            "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 2.052059\tSteps: 235984\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.755487\tSteps: 314384\n",
            "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 1.541598\tSteps: 392784\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.292287\tSteps: 471184\n",
            "Train Epoch: 1 [11200/60000 (19%)]\tLoss: 1.087588\tSteps: 549584\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.019357\tSteps: 627984\n",
            "Train Epoch: 1 [14400/60000 (24%)]\tLoss: 1.037994\tSteps: 706384\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.879296\tSteps: 784784\n",
            "Train Epoch: 1 [17600/60000 (29%)]\tLoss: 0.838662\tSteps: 863184\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.729116\tSteps: 941584\n",
            "Train Epoch: 1 [20800/60000 (35%)]\tLoss: 0.744256\tSteps: 1019984\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.719520\tSteps: 1098384\n",
            "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.681961\tSteps: 1176784\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.711169\tSteps: 1255184\n",
            "Train Epoch: 1 [27200/60000 (45%)]\tLoss: 0.663922\tSteps: 1333584\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.679479\tSteps: 1411984\n",
            "Train Epoch: 1 [30400/60000 (51%)]\tLoss: 0.623640\tSteps: 1490384\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.573780\tSteps: 1568784\n",
            "Train Epoch: 1 [33600/60000 (56%)]\tLoss: 0.611574\tSteps: 1647184\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.537229\tSteps: 1725584\n",
            "Train Epoch: 1 [36800/60000 (61%)]\tLoss: 0.543526\tSteps: 1803984\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.688800\tSteps: 1882384\n",
            "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.506863\tSteps: 1960784\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.541920\tSteps: 2039184\n",
            "Train Epoch: 1 [43200/60000 (72%)]\tLoss: 0.480161\tSteps: 2117584\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.454338\tSteps: 2195984\n",
            "Train Epoch: 1 [46400/60000 (77%)]\tLoss: 0.515425\tSteps: 2274384\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.458289\tSteps: 2352784\n",
            "Train Epoch: 1 [49600/60000 (83%)]\tLoss: 0.491254\tSteps: 2431184\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.485182\tSteps: 2509584\n",
            "Train Epoch: 1 [52800/60000 (88%)]\tLoss: 0.438358\tSteps: 2587984\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.493261\tSteps: 2666384\n",
            "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.461313\tSteps: 2744784\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.399984\tSteps: 2823184\n",
            "Train Epoch: 1 [59200/60000 (99%)]\tLoss: 0.318562\tSteps: 2901584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.4218, Accuracy: 8584/10000 (86%)\n",
            "\n",
            "Train Epoch: 2 [1600/60000 (3%)]\tLoss: 0.462453\tSteps: 3019184\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.334685\tSteps: 3097584\n",
            "Train Epoch: 2 [4800/60000 (8%)]\tLoss: 0.393604\tSteps: 3175984\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.425700\tSteps: 3254384\n",
            "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.441730\tSteps: 3332784\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.447608\tSteps: 3411184\n",
            "Train Epoch: 2 [11200/60000 (19%)]\tLoss: 0.355446\tSteps: 3489584\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.358223\tSteps: 3567984\n",
            "Train Epoch: 2 [14400/60000 (24%)]\tLoss: 0.393927\tSteps: 3646384\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.350214\tSteps: 3724784\n",
            "Train Epoch: 2 [17600/60000 (29%)]\tLoss: 0.360755\tSteps: 3803184\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.323068\tSteps: 3881584\n",
            "Train Epoch: 2 [20800/60000 (35%)]\tLoss: 0.298745\tSteps: 3959984\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.261055\tSteps: 4038384\n",
            "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.373756\tSteps: 4116784\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.327722\tSteps: 4195184\n",
            "Train Epoch: 2 [27200/60000 (45%)]\tLoss: 0.311971\tSteps: 4273584\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.291641\tSteps: 4351984\n",
            "Train Epoch: 2 [30400/60000 (51%)]\tLoss: 0.311335\tSteps: 4430384\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.269205\tSteps: 4508784\n",
            "Train Epoch: 2 [33600/60000 (56%)]\tLoss: 0.281427\tSteps: 4587184\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.234630\tSteps: 4665584\n",
            "Train Epoch: 2 [36800/60000 (61%)]\tLoss: 0.240092\tSteps: 4743984\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.324257\tSteps: 4822384\n",
            "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.243693\tSteps: 4900784\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.245902\tSteps: 4979184\n",
            "Train Epoch: 2 [43200/60000 (72%)]\tLoss: 0.256763\tSteps: 5057584\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.208281\tSteps: 5135984\n",
            "Train Epoch: 2 [46400/60000 (77%)]\tLoss: 0.243355\tSteps: 5214384\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.280237\tSteps: 5292784\n",
            "Train Epoch: 2 [49600/60000 (83%)]\tLoss: 0.300052\tSteps: 5371184\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.280817\tSteps: 5449584\n",
            "Train Epoch: 2 [52800/60000 (88%)]\tLoss: 0.232402\tSteps: 5527984\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.261954\tSteps: 5606384\n",
            "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.246489\tSteps: 5684784\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.220123\tSteps: 5763184\n",
            "Train Epoch: 2 [59200/60000 (99%)]\tLoss: 0.128915\tSteps: 5841584\n",
            "\n",
            "Test set: Average loss: 0.3335, Accuracy: 8984/10000 (90%)\n",
            "\n",
            "Train Epoch: 3 [1600/60000 (3%)]\tLoss: 0.254966\tSteps: 5959184\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.154455\tSteps: 6037584\n",
            "Train Epoch: 3 [4800/60000 (8%)]\tLoss: 0.245534\tSteps: 6115984\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.256688\tSteps: 6194384\n",
            "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.224301\tSteps: 6272784\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.263437\tSteps: 6351184\n",
            "Train Epoch: 3 [11200/60000 (19%)]\tLoss: 0.219063\tSteps: 6429584\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.225620\tSteps: 6507984\n",
            "Train Epoch: 3 [14400/60000 (24%)]\tLoss: 0.253611\tSteps: 6586384\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.247886\tSteps: 6664784\n",
            "Train Epoch: 3 [17600/60000 (29%)]\tLoss: 0.238813\tSteps: 6743184\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.208771\tSteps: 6821584\n",
            "Train Epoch: 3 [20800/60000 (35%)]\tLoss: 0.180837\tSteps: 6899984\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.183855\tSteps: 6978384\n",
            "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.199596\tSteps: 7056784\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.199471\tSteps: 7135184\n",
            "Train Epoch: 3 [27200/60000 (45%)]\tLoss: 0.264265\tSteps: 7213584\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.245666\tSteps: 7291984\n",
            "Train Epoch: 3 [30400/60000 (51%)]\tLoss: 0.232997\tSteps: 7370384\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.234615\tSteps: 7448784\n",
            "Train Epoch: 3 [33600/60000 (56%)]\tLoss: 0.198132\tSteps: 7527184\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.206138\tSteps: 7605584\n",
            "Train Epoch: 3 [36800/60000 (61%)]\tLoss: 0.202319\tSteps: 7683984\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.229988\tSteps: 7762384\n",
            "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.198323\tSteps: 7840784\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.185132\tSteps: 7919184\n",
            "Train Epoch: 3 [43200/60000 (72%)]\tLoss: 0.184718\tSteps: 7997584\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.130066\tSteps: 8075984\n",
            "Train Epoch: 3 [46400/60000 (77%)]\tLoss: 0.203447\tSteps: 8154384\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.225941\tSteps: 8232784\n",
            "Train Epoch: 3 [49600/60000 (83%)]\tLoss: 0.223014\tSteps: 8311184\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.220970\tSteps: 8389584\n",
            "Train Epoch: 3 [52800/60000 (88%)]\tLoss: 0.193143\tSteps: 8467984\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.189071\tSteps: 8546384\n",
            "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.177849\tSteps: 8624784\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.184967\tSteps: 8703184\n",
            "Train Epoch: 3 [59200/60000 (99%)]\tLoss: 0.100814\tSteps: 8781584\n",
            "\n",
            "Test set: Average loss: 0.1869, Accuracy: 9445/10000 (94%)\n",
            "\n",
            "Train Epoch: 4 [1600/60000 (3%)]\tLoss: 0.191010\tSteps: 8899184\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.133394\tSteps: 8977584\n",
            "Train Epoch: 4 [4800/60000 (8%)]\tLoss: 0.208024\tSteps: 9055984\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.211967\tSteps: 9134384\n",
            "Train Epoch: 4 [8000/60000 (13%)]\tLoss: 0.195379\tSteps: 9212784\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.183099\tSteps: 9291184\n",
            "Train Epoch: 4 [11200/60000 (19%)]\tLoss: 0.200397\tSteps: 9369584\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.228513\tSteps: 9447984\n",
            "Train Epoch: 4 [14400/60000 (24%)]\tLoss: 0.245712\tSteps: 9526384\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.199653\tSteps: 9604784\n",
            "Train Epoch: 4 [17600/60000 (29%)]\tLoss: 0.200267\tSteps: 9683184\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.168880\tSteps: 9761584\n",
            "Train Epoch: 4 [20800/60000 (35%)]\tLoss: 0.148310\tSteps: 9839984\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.164436\tSteps: 9918384\n",
            "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.197318\tSteps: 9996784\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.189096\tSteps: 10075184\n",
            "Train Epoch: 4 [27200/60000 (45%)]\tLoss: 0.213058\tSteps: 10153584\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.248894\tSteps: 10231984\n",
            "Train Epoch: 4 [30400/60000 (51%)]\tLoss: 0.189733\tSteps: 10310384\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.210902\tSteps: 10388784\n",
            "Train Epoch: 4 [33600/60000 (56%)]\tLoss: 0.182476\tSteps: 10467184\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.201036\tSteps: 10545584\n",
            "Train Epoch: 4 [36800/60000 (61%)]\tLoss: 0.169589\tSteps: 10623984\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.202511\tSteps: 10702384\n",
            "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.242210\tSteps: 10780784\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.195196\tSteps: 10859184\n",
            "Train Epoch: 4 [43200/60000 (72%)]\tLoss: 0.185903\tSteps: 10937584\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.137028\tSteps: 11015984\n",
            "Train Epoch: 4 [46400/60000 (77%)]\tLoss: 0.209867\tSteps: 11094384\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.182539\tSteps: 11172784\n",
            "Train Epoch: 4 [49600/60000 (83%)]\tLoss: 0.215411\tSteps: 11251184\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.193245\tSteps: 11329584\n",
            "Train Epoch: 4 [52800/60000 (88%)]\tLoss: 0.271003\tSteps: 11407984\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.364338\tSteps: 11486384\n",
            "Train Epoch: 4 [56000/60000 (93%)]\tLoss: 0.184546\tSteps: 11564784\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.177629\tSteps: 11643184\n",
            "Train Epoch: 4 [59200/60000 (99%)]\tLoss: 0.083797\tSteps: 11721584\n",
            "\n",
            "Test set: Average loss: 0.1651, Accuracy: 9494/10000 (95%)\n",
            "\n",
            "Train Epoch: 5 [1600/60000 (3%)]\tLoss: 0.209363\tSteps: 11839184\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.133165\tSteps: 11917584\n",
            "Train Epoch: 5 [4800/60000 (8%)]\tLoss: 0.170627\tSteps: 11995984\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.173454\tSteps: 12074384\n",
            "Train Epoch: 5 [8000/60000 (13%)]\tLoss: 0.204853\tSteps: 12152784\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.200521\tSteps: 12231184\n",
            "Train Epoch: 5 [11200/60000 (19%)]\tLoss: 0.158263\tSteps: 12309584\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.161698\tSteps: 12387984\n",
            "Train Epoch: 5 [14400/60000 (24%)]\tLoss: 0.161285\tSteps: 12466384\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.211408\tSteps: 12544784\n",
            "Train Epoch: 5 [17600/60000 (29%)]\tLoss: 0.176518\tSteps: 12623184\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.149121\tSteps: 12701584\n",
            "Train Epoch: 5 [20800/60000 (35%)]\tLoss: 0.257342\tSteps: 12779984\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.287303\tSteps: 12858384\n",
            "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.479500\tSteps: 12936784\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.279695\tSteps: 13015184\n",
            "Train Epoch: 5 [27200/60000 (45%)]\tLoss: 0.214250\tSteps: 13093584\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.186935\tSteps: 13171984\n",
            "Train Epoch: 5 [30400/60000 (51%)]\tLoss: 0.186549\tSteps: 13250384\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.187417\tSteps: 13328784\n",
            "Train Epoch: 5 [33600/60000 (56%)]\tLoss: 0.170355\tSteps: 13407184\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.150974\tSteps: 13485584\n",
            "Train Epoch: 5 [36800/60000 (61%)]\tLoss: 0.169870\tSteps: 13563984\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.254158\tSteps: 13642384\n",
            "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.208260\tSteps: 13720784\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.156177\tSteps: 13799184\n",
            "Train Epoch: 5 [43200/60000 (72%)]\tLoss: 0.182199\tSteps: 13877584\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.142651\tSteps: 13955984\n",
            "Train Epoch: 5 [46400/60000 (77%)]\tLoss: 0.231160\tSteps: 14034384\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.206367\tSteps: 14112784\n",
            "Train Epoch: 5 [49600/60000 (83%)]\tLoss: 0.191247\tSteps: 14191184\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.193994\tSteps: 14269584\n",
            "Train Epoch: 5 [52800/60000 (88%)]\tLoss: 0.188108\tSteps: 14347984\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.157439\tSteps: 14426384\n",
            "Train Epoch: 5 [56000/60000 (93%)]\tLoss: 0.171275\tSteps: 14504784\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.146192\tSteps: 14583184\n",
            "Train Epoch: 5 [59200/60000 (99%)]\tLoss: 0.109694\tSteps: 14661584\n",
            "\n",
            "Test set: Average loss: 0.1724, Accuracy: 9445/10000 (94%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "model_save_name = 'MNIST_classifier.pt'\n",
        "path = F\"/content/drive/My Drive/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "metadata": {
        "id": "ZKrP91MP3NbG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = datasets.MNIST(root=root, train=False, download=True)\n",
        "t_set = datasets.MNIST(root=root, train=False, download=True,\n",
        "                              transform=transforms.Compose([\n",
        "                                  transforms.ToTensor(),\n",
        "                                  transforms.Normalize((0.1307,), (0.3081,))\n",
        "                              ]))\n",
        "data = t_set[4][0].cuda()\n",
        "data = data.view(-1, input_channels, seq_length)\n",
        "output = model(data)\n",
        "pred = output.data.max(1, keepdim=True)[1]\n",
        "\n",
        "display(test_set[4][0])\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        },
        "id": "UPNNt2JuNjDk",
        "outputId": "37ed4327-bd86-408a-ae78-31ca15803787"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F9293980A60>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA2klEQVR4nGNgGLzA6AGU4SYLZTAhJN3ZoQy/bgxJFi8Y64wWN7qko+V8KEtIiwvNRt03N3mgzAN/RNEkV3w3hWn8/xdNMuTTZRiz9+9eVlTJlX+yoCyFF7+cUOX4H/6BMdv+wM2AupZdegVMRJnhCppzOM9cFIKwxP7+zYaJskCo73eDt/YxMDDoKMv/Z/iPppNBc9XXP3/+/Hnx/PefP5wwQUa4tKEyAwPDGoaF0TDTsID6P3900exEAEZGhss4Jf8jOYcJXZKD4QdOKxlevMnHLbnZCbcclQAA/k48Hcv/z+EAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4]], device='cuda:0')\n"
          ]
        }
      ]
    }
  ]
}